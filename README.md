Duper Artifact
-

This directory includes four main items:
- The source code of Duper v0.0.9: This can be used to generate the executables `duper` and `duperPlus` (contained in `duper/.lake/build/bin`) which were used to perform the experiments described in Section 6. To generate the executable `duper` (which corresponds with "Duper (-)" in the paper), simply run `lake build` in the `duper` directory. To generate the executable `duperPlus` (which corresponds with "Duper (+)" in the paper), change `includeExpensiveRules false` to `includeExpensiveRules true` on line 99 of `duper/Main.lean` and run `lake build`. The executable that is subsequently created will still be called `duper`, but we recommend renaming it to `duperPlus` for the sake of scripts that run both `duper` and `duperPlus`.
- A directory called lean-auto: In addition to implementing LeanAuto, described in Section 5, this directory is used to build an executable `defaultExe` in `lean-auto/build/bin` which is used in a variety of ways by the scripts used to run the experiments described in Section 6. Since `defaultExe` is used in different ways by different scripts, the file `lean-auto/Auto/Main.lean` has multiple main functions (some of which are commented out) corresponding to different purposes. Each of these main functions is paired with a comment explaining what said main function does. To build `defaultExe` with the appropriate main function, uncomment the desired main function and run `lake build` in the `lean-auto` directory.
- The full results of Section 6's experiments: The directory `Duper Evaluation/Results` contains all the .txt files generated over the course of Section 6's experiments. Some of these files' names have been changed to better clarify the nature of the file (e.g. `duper_minus_grunge_fof_results.txt` was originally generated as `duper_grunge_results1.txt`).
- The scripts used to run Section 6's experiments: The directory `Duper Evaluation/Scripts` contains all of the scripts used to run Section 6's experiments. Some of these scripts are self-explanatory, while others require additional explanation. Scripts requiring additional explanation are described in further detail later in this README file.

This directory does not include:
- The original Seventeen or GRUNGE benchmarks described in Section 6.1.1: The original GRUNGE benchmarks can be downloaded [here](http://www.tptp.org/CASC/27/TrainingData.HL4.tgz) and the original Seventeen benchmarks can be downloaded as max_facts_probs.zip [here](https://zenodo.org/records/6394938).
- The directories `lean-auto/build` and `duper/.lake`: These directories are generated by calling `lake build` in `lean-auto` and `duper` respectively. Attempting to include these directories in this artifact resulted in the .tgz file exceeding the maximum file size.
- Any of the tools used in Section 6's evaluation besides Duper: Metis, Vampire, and Zipperposition are all available online and the specific versions we use for each of them are described in Section 6.1.2 of the paper.

Additional information about the test scripts:
- All of the test scripts (as well as `lean-auto/Auto/Main.lean`) have hardcoded paths which should be modified for your use case.
- Many of the Seventeen benchmark scripts presume the existence of a file called `all_problems.txt` in the directory where the script is run. The script `(find . -type f) > all_problems.txt` (which is listed in command_to_generate_all_problems.txt) is used to generate this file and must be run before any of the scripts used for evaluating on the Seventeen benchmarks.
- The original Seventeen benchmarks contain trailing comments that Metis' parser cannot handle on its own. To sidestep this issue and run Metis on the 17 Provers benchmarks anyway, we first ran `cleanup_17_benchmarks.sh` on each relevant subdirectory (e.g. `17 Provers max_facts_probs/FOF_0016`) to eliminate all trailing comments. This script assumes the existence of a duplicate subdirectory "clean" which can be generated via the command `cp -r . ../clean; mv ../clean clean`.
- The scripts used to run Zipperposition's experiments on the Seventeen benchmarks don't just record the set of problems Zipperposition can solve. They also output the solution created by Zipperposition and a minimized problem. This requires that, in the same directory that contains the Seventeen benchmarks, there are duplicate subdirectories "clean", "bushy", and "zipperposition_solution". These duplicate subdirectories can be generated via commands such as `cp -r . ../clean; mv ../clean clean`.
- The scripts used to run Vampire's experiments on the Seventeen benchmarks don't just record the set of problems Vampire can solve. They also output the solution created by Vampire (but not a minimized problem). This requires that, in the same directory that contains the Seventeen benchmarks, there are duplicate subdirectories "clean" and "vampire_solution". These duplicate subdirectories can be generated via commands such as `cp -r . ../clean; mv ../clean clean`.
- To generate Vampire minimized problems, the scripts `get_hyps_from_vampire_output.sh` and `make_vampire_minimized_17_benchmarks.sh` are included. The former script looks at the Vampire output in the "vampire_solution" subdirectory and generates files in "vampire_hyps" containing the list of axioms used, and the latter script looks at that output and the original benchmark problem to generate vampire minimized problems. Collectively, these scripts assume the existence of duplicate subdirectories "clean", "vampire_hyps", and "vampire_bushy", which can be generated via commands such as `cp -r . ../clean; mv ../clean clean`.
- We acknowledge that these test scripts can be difficult to navigate. Our intention is that this README contains all of the information necessary to understand the test scripts well enough to replicate our results, but if you encounter ambiguities that make it difficult to replicate our experiments, we encourage you to reach out to the first author at jclune@andrew.cmu.edu.